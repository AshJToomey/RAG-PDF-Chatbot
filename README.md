# RAG PDF Chatbot with LangChain and Ollama

This project demonstrates a lightweight Retrieval-Augmented Generation (RAG) chatbot built using LangChain. It loads a PDF document, splits the content, embeds it using HuggingFace embeddings, and uses FAISS as the vector store. Responses are generated using a locally running LLM via Ollama (e.g., `llama3`).

---

## ğŸ”§ Tech Stack

- **LangChain** â€” RAG orchestration
- **HuggingFace Embeddings** â€” For semantic search (no API key required)
- **FAISS** â€” Vector store for efficient retrieval
- **Ollama** â€” Local LLM interface (e.g. `llama3`)
- **PDF Loader** â€” Reads and chunks documents for ingestion

---

## ğŸš€ How It Works

1. Loads and splits a PDF file (`CV_for_Ashley.pdf`)
2. Generates document embeddings via HuggingFace
3. Stores embeddings in FAISS (in-memory)
4. Runs a LangChain `retrieval_qa` chain using an Ollama-hosted LLM
5. Accepts user queries and returns LLM-generated responses grounded in the PDF

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ main.py                  # Main script
â”œâ”€â”€ CV_for_Ashley.pdf       # Sample PDF for demo
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # Project overview (this file)
```

---

## ğŸ§ª Usage

### 1. Start Ollama
Install [Ollama](https://ollama.com/) and run:
```bash
ollama run llama3
```

### 2. Install Python Dependencies
```bash
pip install -r requirements.txt
```

### 3. Run the App
```bash
python main.py
```

### 4. Sample Output
```bash
> What is this paper about?
This document describes Ashley's qualifications, experience, and education...
```

---

## ğŸ“¦ Requirements

- Python 3.8+
- Ollama installed locally
- PDF file (e.g. `CV_for_Ashley.pdf`)

---

## ğŸ“ Notes
- The vectorstore is created in-memory each time the script runs. For persistence, you can use:
  ```python
  vectorstore.save_local("vectorstore_index")
  ```

---

## ğŸ“„ License
MIT â€” feel free to use and modify!
